{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "064cba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import docx\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "32ea0b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khpat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\khpat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khpat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khpat\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b48eb77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text(file_path):\n",
    "    file_path_obj = open(file_path, 'rb')\n",
    "    file_obj = PyPDF2.PdfReader(file_path_obj)\n",
    "\n",
    "    print(len(file_obj.pages))\n",
    "    page_obj = file_obj.pages[0]\n",
    "    print(page_obj.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5dc1330e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Welcome to Smallpdf\n",
      "Digital Documents—All In One Place\n",
      "Access Files Anytime, Anywhere Enhance Documents in One Click \n",
      "Collaborate With Others With the new Smallpdf experience, you can \n",
      "freely upload, organize, and share digital \n",
      "documents. When you enable the ‘Storage’ \n",
      "option, we’ll also store all processed files here. \n",
      "You can access files stored on Smallpdf from \n",
      "your computer, phone, or tablet. We’ll also \n",
      "sync files from the Smallpdf Mobile App to our \n",
      "online portalWhen you right-click on a file, we’ll present \n",
      "you with an array of options to convert, \n",
      "compress, or modify it. \n",
      "Forget mundane administrative tasks. With \n",
      "Smallpdf, you can request e-signatures, send \n",
      "large files, or even enable the Smallpdf G Suite \n",
      "App for your entire organization. Ready to take document management to the next level? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_pdf_text('pdf_7.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9e89c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_doc(output_path):\n",
    "    doc = docx.Document()\n",
    "    doc.add_heading(\"Heading 0\", 0)\n",
    "    para = doc.add_paragraph(\"Paragraph : \")\n",
    "    para.add_run(\"Bold\").bold = True\n",
    "    para.add_run(\"Italic\").italic = True\n",
    "    doc.add_page_break()\n",
    "    doc.add_heading(\"Heading 2\", 2)\n",
    "    doc.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c69ea1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_word_doc(\"new_doc.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7f003185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization():\n",
    "    sent = \"The First sentence is about Python. The Second: about Django. You can learn Python,Django and Data Analysis here.\"\n",
    "    print(sent_tokenize(sent))\n",
    "\n",
    "    word = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "    print(word_tokenize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aadf5335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The First sentence is about Python.', 'The Second: about Django.', 'You can learn Python,Django and Data Analysis here.']\n",
      "['It', 'originated', 'from', 'the', 'idea', 'that', 'there', 'are', 'readers', 'who', 'prefer', 'learning', 'new', 'skills', 'from', 'the', 'comforts', 'of', 'their', 'drawing', 'rooms']\n"
     ]
    }
   ],
   "source": [
    "tokenization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9b7fd53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('welcome', 'NN'), ('to', 'TO'), ('the', 'DT'), ('world', 'NN'), ('of', 'IN'), ('to', 'TO'), ('learn', 'VB'), ('Categorizing', 'NNP'), ('and', 'CC'), ('POS', 'NNP'), ('Tagging', 'NNP'), ('with', 'IN'), ('NLTK', 'NNP'), ('and', 'CC'), ('Python', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "text = word_tokenize(\"Hello welcome to the world of to learn Categorizing and POS Tagging with NLTK and Python\")\n",
    "print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7a1ab83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_token = word_tokenize(example_sent)\n",
    "filtered_sent = [w for w in word_token if w.lower() not in stop_words]\n",
    "print(filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b09438ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It : It\n",
      "vijaying : vijay\n",
      "meeting : meet\n",
      "better : better\n",
      "vijayed : vijay\n",
      "vijays : vijay\n",
      "eats : eat\n",
      "skills : skill\n",
      "originated : origin\n",
      "from : from\n",
      "the : the\n",
      "idea : idea\n",
      "that : that\n",
      "there : there\n",
      "are : are\n",
      "readers : reader\n",
      "who : who\n",
      "prefer : prefer\n",
      "learning : learn\n",
      "new : new\n",
      "skills : skill\n",
      "from : from\n",
      "the : the\n",
      "comforts : comfort\n",
      "of : of\n",
      "their : their\n",
      "drawing : draw\n",
      "rooms : room\n",
      "It : It\n",
      "vijaying : vijaying\n",
      "meeting : meeting\n",
      "better : better\n",
      "vijayed : vijayed\n",
      "vijays : vijays\n",
      "eats : eats\n",
      "skills : skill\n",
      "originated : originated\n",
      "from : from\n",
      "the : the\n",
      "idea : idea\n",
      "that : that\n",
      "there : there\n",
      "are : are\n",
      "readers : reader\n",
      "who : who\n",
      "prefer : prefer\n",
      "learning : learning\n",
      "new : new\n",
      "skills : skill\n",
      "from : from\n",
      "the : the\n",
      "comforts : comfort\n",
      "of : of\n",
      "their : their\n",
      "drawing : drawing\n",
      "rooms : room\n"
     ]
    }
   ],
   "source": [
    "word_data = \"It vijaying meeting better vijayed vijays eats skills originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "token = word_tokenize(word_data)\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for w in token:\n",
    "    print(f\"{w} : {stemmer.stem(w)}\")\n",
    "for w in token:\n",
    "    print(f\"{w} : {lemmatizer.lemmatize(w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5472b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a0bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = \"Data Science is the best job of the 21st century\"\n",
    "second_sentence = \"Machine learning is the key for data science\"\n",
    "\n",
    "#split so each word have their own string\n",
    "first_sentence = first_sentence.split(\" \")\n",
    "second_sentence = second_sentence.split(\" \")#join them to remove common duplicate words\n",
    "total= set(first_sentence).union(set(second_sentence))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee2f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the words\n",
    "\n",
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in second_sentence:\n",
    "    wordDictB[word]+=1\n",
    "    \n",
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec47cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Term Frequency(TF)\n",
    "\n",
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "\n",
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_sentence)\n",
    "tfSecond = computeTF(wordDictB, second_sentence)\n",
    "\n",
    "#Converting to dataframe for visualization\n",
    "pd.DataFrame([tfFirst, tfSecond])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d6028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Inverse Document Frequency(IDF)\n",
    "\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) + 1))\n",
    "        \n",
    "    return(idfDict)\n",
    "\n",
    "#inputing our sentences in the log file\n",
    "idfs = computeIDF([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc98bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Term Frequency(TF) - Inverse Document Frequency(IDF)\n",
    "\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "\n",
    "#running our two sentences through the IDF:\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "\n",
    "#putting it in a dataframe\n",
    "pd.DataFrame([idfFirst, idfSecond])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90604f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute TF-IDF\n",
    "\n",
    "#first step is to import the library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#for the sentence, make sure all words are lowercase or you will run #into error. for simplicity, I just made the same sentence all #lowercase\n",
    "firstV= \"Data Science is the sexiest job of the 21st century\"\n",
    "secondV= \"machine learning is the key for data science\"\n",
    "\n",
    "#calling the TfidfVectorizer\n",
    "vectorize= TfidfVectorizer()\n",
    "\n",
    "#fitting the model and passing our sentences right away:\n",
    "response= vectorize.fit_transform([firstV, secondV])\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
